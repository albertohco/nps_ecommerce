# üöÄ Otimiza√ß√µes de Performance

## Modelo Ollama Otimizado

O sistema agora usa o modelo **gemma:2b** para melhor performance:

### Instalar o modelo otimizado:
```bash
ollama pull gemma:2b
```

### Por que gemma:2b?
- **Mais r√°pido**: ~2-3x mais r√°pido que phi3
- **Menor**: ~1.7GB vs ~2.3GB do phi3
- **Eficiente**: √ìtimo para tarefas de classifica√ß√£o simples

### Compara√ß√£o de modelos:

| Modelo | Tamanho | Velocidade | Qualidade |
|--------|---------|------------|-----------|
| gemma:2b | 1.7GB | ‚ö°‚ö°‚ö° R√°pido | ‚úÖ Boa |
| phi3 | 2.3GB | ‚ö°‚ö° M√©dio | ‚úÖ‚úÖ Muito boa |
| llama2 | 3.8GB | ‚ö° Lento | ‚úÖ‚úÖ‚úÖ Excelente |

### Outras otimiza√ß√µes implementadas:

1. **Processamento unit√°rio**: 1 avalia√ß√£o por vez para feedback instant√¢neo
2. **Prompt simplificado**: Resposta mais direta e r√°pida
3. **Temperature=0**: Respostas consistentes e r√°pidas
4. **Limite de tokens**: Apenas 5 tokens de resposta
5. **Sem delays**: Processamento cont√≠nuo sem pausas

### Trocar de modelo (se necess√°rio):

Edite `backend.py` linha 65:
```python
def get_ollama_sentiment_score(texto: str, model: str = "gemma:2b"):
```

Modelos dispon√≠veis:
- `gemma:2b` - Recomendado (r√°pido)
- `phi3` - Balanceado
- `llama2` - Mais preciso (lento)
- `mistral` - Alternativa r√°pida
